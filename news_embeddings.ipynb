{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining \n",
    "\n",
    "For Data Mining Lab (2017 Fall)\n",
    "\n",
    "Lectured by Elvis Saravia (Website | Twitter)\n",
    "\n",
    "Accompanying slides here (Link)\n",
    "\n",
    "Reference slides (Link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook I will explore a text-based, document-based [dataset](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) using scientific computing tools such as Pandas and Numpy. In addition, several fundamental Data Mining concepts will be explored and explained in details, ranging from calculating distance measures to computing term frequency vectors to visualizing embeddings. Coding examples, visualizations and demonstrations will be provided where necessary. Furthermore, additional exercises are provided after special topics. These exercises are geared towards testing the understanding of students and also for students to explore beyond, through programming practises, the techniques covered in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "Here are the computing and software requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary for when working with external scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellfae/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# my functions\n",
    "import helpers.word2vec_helpers as wh\n",
    "import helpers.word_embeddings as we\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import helpers.text_analysis as ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data source\n",
    "In this notebook we will explore the popular 20 newsgroup dataset, originally provided [here](http://qwone.com/~jason/20Newsgroups/). The dataset is called \"Twenty Newsgroups\", which means there are 20 categories of news articles available in the entire dataset. A short description of the dataset, provided by the authors, is provided below:\n",
    "\n",
    "- *The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.*\n",
    "\n",
    "If you need more information about the dataset please refer to the reference provided above. Below is a snapshot of the dataset already converted into a table. Keep in mind that the original dataset is not in this nice pretty format. That work is left to us. That is one of the tasks that will be covered in this notebook: how to convert raw data into convenient tabular formats using Pandas. \n",
    "\n",
    "![atl txt](https://docs.google.com/drawings/d/e/2PACX-1vRd845nNXa1x1Enw6IoEbg-05lB19xG3mfO2BjnpZrloT0pSnY89stBV1gS9Iu6cgRCTq3E5giIT5ZI/pub?w=835&h=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "Now let us begin to explore the data. The original dataset can be found on the link provided above or you can directly use the version provided by scikit learn. Here we will use the scikit learn version. \n",
    "\n",
    "In this demonstration we are only going to look at 4 categories. This means we will not make use of the complete dataset, but only a subset of it, which includes the 4 categories defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the documents containing the categories provided\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, \\\n",
    "                                  shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take at look some of the records that are contained in our subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n',\n",
       " \"From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the 'gurus' can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the `twenty_train` is just a bunch of objects that can be accessed as python dictionaries; so, you can do the following operations on `twenty_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also print an example from the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example of what the subset contains\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and determine the label of the example via `target_names` key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we can also get the category of 10 documents via `target` key value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category of first 10 documents.\n",
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: As you can observe, both approaches above provide two different ways of obtaining the `category` value for the dataset. Ideally, we want to have access to both types -- numerical and nominal -- in the event some particular library favors a particular type. \n",
    "\n",
    "As you may have already noticed as well, there is no **tabular format** for the current version of the data. As data miners, we are interested in having our dataset in the most convenient format as possible; something we can manipulate easily and is compatible with our algorithms, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one way to get access to the *text* version of the label of a subset of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. So What's next?\n",
    "So we want to explore and understand our data a little bit better. Before we do that we definitely need to make some conversions here and there just so we can have our dataset in a nice format to be able to explore it freely. Lucky for us, there are efficient scientific tools to transform our data into that tabular format we are so farmiliar with. So that is what we will do in the next section--transform our data into a nice table to play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Converting Dictionary into Pandas dataframe\n",
    "**Note:** This part is not provided in the original scikit learn tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n',\n",
       " \"From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the 'gurus' can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame.from_records(dmh.format_rows(twenty_train), columns= ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category to the dataframe\n",
    "X['category'] = twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category label also\n",
    "X['category_name'] = X.category.apply(lambda t: dmh.format_labels(t, twenty_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print screen and see what our table looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart) Subject...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: jodfishe@silver.ucs.indiana.edu (joseph ...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: aldridge@netcom.com (Jacquelin Aldridge)...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: geb@cs.pitt.edu (Gordon Banks) Subject: ...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: libman@hsc.usc.edu (Marlena Libman) Subj...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  \\\n",
       "0  From: sd345@city.ac.uk (Michael Collier) Subje...         1   \n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...         1   \n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...         3   \n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart) Subject...         3   \n",
       "4  From: stanly@grok11.columbiasc.ncr.com (stanly...         3   \n",
       "5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...         3   \n",
       "6  From: jodfishe@silver.ucs.indiana.edu (joseph ...         3   \n",
       "7  From: aldridge@netcom.com (Jacquelin Aldridge)...         2   \n",
       "8  From: geb@cs.pitt.edu (Gordon Banks) Subject: ...         2   \n",
       "9  From: libman@hsc.usc.edu (Marlena Libman) Subj...         2   \n",
       "\n",
       "            category_name  \n",
       "0           comp.graphics  \n",
       "1           comp.graphics  \n",
       "2  soc.religion.christian  \n",
       "3  soc.religion.christian  \n",
       "4  soc.religion.christian  \n",
       "5  soc.religion.christian  \n",
       "6  soc.religion.christian  \n",
       "7                 sci.med  \n",
       "8                 sci.med  \n",
       "9                 sci.med  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Isn't it? With this format of the data we can conduct many operations, easily and efficiently, since Pandas dataframes provide us with a wide range of built-in features. These features are operations which can directly and quickly be applied to the dataset. These operations may include standard operations like removing records with missing values and aggregating new fields to the current state of the table (hereinafter referred to as a dataframe), which is desirable in almost every data mining project. If you are not covinced head on over to Kaggle -- one of the largest data science community on the web -- and see for yourself some of the top kernels and what they make use of. First thing you will notice is the use of Pandas to transform and explore data. Take a look at this [kernel](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) for example.\n",
    "\n",
    "I want to also add here that in this notebook we are not following conventions, but we are very interested in using tools and techniques that make our lives easier. Go Pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Familiarizing yourself with the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin to show you the magnificence of Pandas dataframes, let us look at how to run a simple query on our dataset. We want to query for the first 10 rows (news articles), and we only want to keep the `text` and `category_name` attributes or fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart) Subject...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: jodfishe@silver.ucs.indiana.edu (joseph ...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: aldridge@netcom.com (Jacquelin Aldridge)...</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: geb@cs.pitt.edu (Gordon Banks) Subject: ...</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: libman@hsc.usc.edu (Marlena Libman) Subj...</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           category_name\n",
       "0  From: sd345@city.ac.uk (Michael Collier) Subje...           comp.graphics\n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...           comp.graphics\n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...  soc.religion.christian\n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart) Subject...  soc.religion.christian\n",
       "4  From: stanly@grok11.columbiasc.ncr.com (stanly...  soc.religion.christian\n",
       "5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...  soc.religion.christian\n",
       "6  From: jodfishe@silver.ucs.indiana.edu (joseph ...  soc.religion.christian\n",
       "7  From: aldridge@netcom.com (Jacquelin Aldridge)...                 sci.med\n",
       "8  From: geb@cs.pitt.edu (Gordon Banks) Subject: ...                 sci.med\n",
       "9  From: libman@hsc.usc.edu (Marlena Libman) Subj...                 sci.med"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:10][[\"text\", \"category_name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a few more interesting queries to familiarize ourselves with the efficiency and conveniency of Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want the last 10 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>From: lmvec@westminster.ac.uk (William Hargrea...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>From: daniels@math.ufl.edu (TV's Big Dealer) S...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>From: \"danny hawrysio\" &lt;danny.hawrysio@canrem....</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>From: shellgate!llo@uu4.psi.com (Larry L. Over...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>From: ingles@engin.umich.edu (Ray Ingles) Subj...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>From: Mark-Tarbell@suite.com Subject: Amniocen...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>From: roos@Operoni.Helsinki.FI (Christophe Roo...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>From: mhollowa@ic.sunysb.edu (Michael Holloway...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>From: sasghm@theseus.unx.sas.com (Gary Merrill...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>From: Dan Wallach &lt;dwallach@cs.berkeley.edu&gt; S...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  category  \\\n",
       "2246  From: lmvec@westminster.ac.uk (William Hargrea...         3   \n",
       "2247  From: daniels@math.ufl.edu (TV's Big Dealer) S...         3   \n",
       "2248  From: \"danny hawrysio\" <danny.hawrysio@canrem....         1   \n",
       "2249  From: shellgate!llo@uu4.psi.com (Larry L. Over...         3   \n",
       "2250  From: ingles@engin.umich.edu (Ray Ingles) Subj...         0   \n",
       "2251  From: Mark-Tarbell@suite.com Subject: Amniocen...         2   \n",
       "2252  From: roos@Operoni.Helsinki.FI (Christophe Roo...         2   \n",
       "2253  From: mhollowa@ic.sunysb.edu (Michael Holloway...         2   \n",
       "2254  From: sasghm@theseus.unx.sas.com (Gary Merrill...         2   \n",
       "2255  From: Dan Wallach <dwallach@cs.berkeley.edu> S...         2   \n",
       "\n",
       "               category_name  \n",
       "2246  soc.religion.christian  \n",
       "2247  soc.religion.christian  \n",
       "2248           comp.graphics  \n",
       "2249  soc.religion.christian  \n",
       "2250             alt.atheism  \n",
       "2251                 sci.med  \n",
       "2252                 sci.med  \n",
       "2253                 sci.med  \n",
       "2254                 sci.med  \n",
       "2255                 sci.med  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-11:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready for some sourcery? Brace yourselves! Let us see if we can query every 10th record in our dataframe. In addition, our query must only contain the first 10 records. For this we will use the build-in function ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: anasaz!karl@anasazi.com (Karl Dussik) Su...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>From: dotsonm@dmapub.dma.org (Mark Dotson) Sub...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>From: vgwlu@dunsell.calgary.chevron.com (greg ...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>From: david-s@hsr.no (David A. Sjoen) Subject:...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>From: ab@nova.cc.purdue.edu (Allen B) Subject:...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>From: Nanci Ann Miller &lt;nm0w+@andrew.cmu.edu&gt; ...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>From: weaver@chdasic.sps.mot.com (Dave Weaver)...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>From: annick@cortex.physiol.su.oz.au (Annick A...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Subject: Vonnegut/atheism From: dmn@kepler.unh...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  category  \\\n",
       "0   From: sd345@city.ac.uk (Michael Collier) Subje...         1   \n",
       "10  From: anasaz!karl@anasazi.com (Karl Dussik) Su...         3   \n",
       "20  From: dotsonm@dmapub.dma.org (Mark Dotson) Sub...         3   \n",
       "30  From: vgwlu@dunsell.calgary.chevron.com (greg ...         2   \n",
       "40  From: david-s@hsr.no (David A. Sjoen) Subject:...         3   \n",
       "50  From: ab@nova.cc.purdue.edu (Allen B) Subject:...         1   \n",
       "60  From: Nanci Ann Miller <nm0w+@andrew.cmu.edu> ...         0   \n",
       "70  From: weaver@chdasic.sps.mot.com (Dave Weaver)...         3   \n",
       "80  From: annick@cortex.physiol.su.oz.au (Annick A...         2   \n",
       "90  Subject: Vonnegut/atheism From: dmn@kepler.unh...         0   \n",
       "\n",
       "             category_name  \n",
       "0            comp.graphics  \n",
       "10  soc.religion.christian  \n",
       "20  soc.religion.christian  \n",
       "30                 sci.med  \n",
       "40  soc.religion.christian  \n",
       "50           comp.graphics  \n",
       "60             alt.atheism  \n",
       "70  soc.religion.christian  \n",
       "80                 sci.med  \n",
       "90             alt.atheism  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[::10, :][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Do more querying here using pandas dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Mining using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's stop playing around and let's do some serious work. Let's learn to program some of the ideas and concepts related to the data mining course. This is the only way we can be convinced of the true power of Pandas dataframes for data mining tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us consider that our dataset has some *missing values* and we want to remove those values. In its current state our dataset has no missing values, but for practice sake we will add some records with missing values and then write to code to deal with these object that contain missing value. You will see for yourself how easy it is to deatl with missing values once you have your data transformed into a Pandas dataframe.\n",
    "\n",
    "First let us do a quick review of what we learned in the Data Mining course. Specifically, how to deal with missing values and how they come about. \n",
    "\n",
    "The most common of cases for having missing values in datasets has to do with how the data was initially collected. A good example of this is when a patients comes into the ER room, the data is collected as quickly as possible and depending on the conditions of the patients, the personal data being collected it either incomplete or partially complete. In the former and latter cases, we are presented with a case of \"missing values\". Knowing that patients data is particularly critical and can be used by the health authorities to conduct some interesting analysis, we as the data miners are left with the tough task of deciding what to do with these missing and incomplete records. We need to deal with these records because they are definitely going to affect our analysis or algorithms. So what do we do? There are several ways to handle missing values, and some of the more effective ways are presented below (Note: You can reference the slides - Session 1 Handout for the additional information).\n",
    "\n",
    "- **Eliminate Data Objects** - Here we completely discard records once they contain some missing values. This is the easiest approach and the one we will be using in this notebook. The immediate drawback of going with this approach is that you loose information. Now imagine that half of the records have at least one or more missing values. Here you are presented with the tough decision of quantity vs quality. In any event, this decision must be made carefully, hence the reason for emphasizing it here in this notebook. \n",
    "\n",
    "- **Estimate Missing Values** - Here we try to estimate the missing values based on some criteria. Although this approach may be proven to be effective, it is not always the case, especially when we are dealing with sensitive data, like **Gender** or **Names**. For fields like **Address**, there could be ways to obtain these missing addresses using some data aggregation technique or obtain the information directly from other databases or public data sources.\n",
    "\n",
    "- **Ignore the missing value during analysis** - Here we basically ignore the missing values and proceed with our analysis. Although this is the most naive way to handle missing values it may proof effective, especially when the missing values includes information that is not important to the analysis being conducted. But think about it for a while. Would you be ignore missing values, especially when in this day and age it is difficult to obtain high quality databases. Again, there are some tradeoffs, which will talk about later in the notebook.\n",
    "\n",
    "- **Replace with all possible values** - As an efficient and responsible data miner, we sometimes just need to put in the hard hours of work and find ways to makes up for these missing values. This last option is a very wise option for cases where data is scarce (which is almost always) or when dealing with sensitive data. Imagine that our dataset has an **Age** field, which contains many missing values. Since **Age** is a continuous variable, it means that we can build a separate model for calculating the age for the incomplete records based on some rule-based appraoch or probabilistic approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we are going to go with the first option but you may be asked to compute missing values, using a different approach, as an exercise. So let's get to it!\n",
    "\n",
    "First we want to add the dummy records with missing values since the dataset we have is perfectly composed and cleaned that it contains no missing values. First let us check for ourselves that indeed the dataset doesn't contain any missing values. We can do that easily by using the following built-in function provided by Pandas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2257 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  category  category_name\n",
       "0     False     False          False\n",
       "1     False     False          False\n",
       "2     False     False          False\n",
       "3     False     False          False\n",
       "4     False     False          False\n",
       "5     False     False          False\n",
       "6     False     False          False\n",
       "7     False     False          False\n",
       "8     False     False          False\n",
       "9     False     False          False\n",
       "10    False     False          False\n",
       "11    False     False          False\n",
       "12    False     False          False\n",
       "13    False     False          False\n",
       "14    False     False          False\n",
       "15    False     False          False\n",
       "16    False     False          False\n",
       "17    False     False          False\n",
       "18    False     False          False\n",
       "19    False     False          False\n",
       "20    False     False          False\n",
       "21    False     False          False\n",
       "22    False     False          False\n",
       "23    False     False          False\n",
       "24    False     False          False\n",
       "25    False     False          False\n",
       "26    False     False          False\n",
       "27    False     False          False\n",
       "28    False     False          False\n",
       "29    False     False          False\n",
       "...     ...       ...            ...\n",
       "2227  False     False          False\n",
       "2228  False     False          False\n",
       "2229  False     False          False\n",
       "2230  False     False          False\n",
       "2231  False     False          False\n",
       "2232  False     False          False\n",
       "2233  False     False          False\n",
       "2234  False     False          False\n",
       "2235  False     False          False\n",
       "2236  False     False          False\n",
       "2237  False     False          False\n",
       "2238  False     False          False\n",
       "2239  False     False          False\n",
       "2240  False     False          False\n",
       "2241  False     False          False\n",
       "2242  False     False          False\n",
       "2243  False     False          False\n",
       "2244  False     False          False\n",
       "2245  False     False          False\n",
       "2246  False     False          False\n",
       "2247  False     False          False\n",
       "2248  False     False          False\n",
       "2249  False     False          False\n",
       "2250  False     False          False\n",
       "2251  False     False          False\n",
       "2252  False     False          False\n",
       "2253  False     False          False\n",
       "2254  False     False          False\n",
       "2255  False     False          False\n",
       "2256  False     False          False\n",
       "\n",
       "[2257 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `isnull` function looks through the entire dataset for null values and return `True` wherever it finds any missing field or record. As you will see above, and as we anticipated, our dataset looks clean and all values are present, since `isnull` return **False** for all fields and records. But let us start to get our hands dirty and build a nice little function to check each of the records, column by column, and return a nice little message telling us the amount of missing records collected. This excerice will also encourage us to explore the possibilities of pandas dataframes. In reality, the build function are good enough, but as you saw above when the entire table was printed, it is impossible really to tell if there are missing records just by looking at the records manually, especially in cases where the dataset is huge. Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text             (The amoung of missing records is: , 0)\n",
       "category         (The amoung of missing records is: , 0)\n",
       "category_name    (The amoung of missing records is: , 0)\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, a lot happened there in that one line of code, so let's break it down. First, with the `isnull` we tranformed our table into the **True/False** table you see above, where **True** in this case means that the data is missing and **False** means that the data is present. We then take the transformed table and apply a function to each that essentially counts to see if there are missing values in each records and how much missing values are there. In other words the `check_missing_values` function looks through each field in the dataset and counts how many missing values there are. \n",
    "\n",
    "There are many other clever ways to check for missing data, and that is what makes Pandas so beautiful to work with. You get the control you need as a data scientist or just a person working in data mining projects. Indeed, Pandas makes your life easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Insert excercise here on how to calculate the missing values for every record instead of every column. Hint `axis` parameter. Check the documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our function to check for missing records, now let us do some bad deed and insert some dummy data into the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_series = pd.Series([\"dummy_record\", 1], index=[\"text\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        dummy_record\n",
       "category               1\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_series = X.append(dummy_series, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2258"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the records was commited into result\n",
    "len(result_with_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we that we have added the record (specifically a `Series` or what we would usually now as lists) with some missing values. Let try our function and see if it can detect that there is a missing value on the resulting dataframe called ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text             (The amoung of missing records is: , 0)\n",
       "category         (The amoung of missing records is: , 0)\n",
       "category_name    (The amoung of missing records is: , 1)\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_series.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed there is a missing value in this new dataframe. Specificall, the missing value comes from the `category_name` attribute. As I mentioned before, there are many ways to conduct specific operations on the dataframes. In this case let us use a simple dictionary and try to insert it into our original dataframe `X`. Notice also that we are note changing the `X` dataframe as results are directly applied to the assignment variable provided. But in the event that we just want to keep things simple, we can just directly apply the changes to `X` and assign it to itself as we will do below. This exercise will challenge to us for the need to remove this dummy records later on, which means that we need to learn more about Pandas dataframes. This is getting intense! But just relax, everything will be fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dict = [{'text': 'dummy_record',\n",
    "               'category': 1\n",
    "              }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.append(dummy_dict,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2258"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text             (The amoung of missing records is: , 0)\n",
       "category         (The amoung of missing records is: , 0)\n",
       "category_name    (The amoung of missing records is: , 1)\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we can see that our data has missing values, we want to remove the records with missing values. The code to drop the record with missing that we just added, is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now let us test to see if we gotten rid of the records with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text             (The amoung of missing records is: , 0)\n",
       "category         (The amoung of missing records is: , 0)\n",
       "category_name    (The amoung of missing records is: , 0)\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are back with our original dataset, clean and tidy as we want it. That's enough on how to deal with missing values, let us now move unto something more fun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just in case you want to learn more about how to deal with missing data, go to reference the official [Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/missing_data.html#missing-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dealing with Duplicate Data\n",
    "Dealing with duplicate data is just as painful as dealing with missing data. The only difference between the two is that now there is more of the same. The worst case is that you have duplicate data that has missing values. But let us not get carried away. Let us keep it to the basics. As we have learned in our Data Mining course, duplicated data can occur because of many reasons. The majority of the times it has to do with how we store data or how we collect and merge data. For instance, we may collected and stored a tweet, and retweet of that same tweet as two different records. The result is case of data duplication; the only difference being that one is the original tweet and the other the retweeted one. Here you will learn that dealing with duplicate data is not as challenging as missing values. But this also all depends on what you consider as duplicates; this all depends on your criteria for what is considered as a duplicate and also what type of data you are dealing with. For text it may not be so trivial as it is for numerical values. Anyhow let us look at some code on how to deal with duplicate records in our `X` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us check how many duplicates we have in our current dataset. Here is the line of code that checks for duplicates; it is very similar to the `isnull` operation that we used to check for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "5       False\n",
       "6       False\n",
       "7       False\n",
       "8       False\n",
       "9       False\n",
       "10      False\n",
       "11      False\n",
       "12      False\n",
       "13      False\n",
       "14      False\n",
       "15      False\n",
       "16      False\n",
       "17      False\n",
       "18      False\n",
       "19      False\n",
       "20      False\n",
       "21      False\n",
       "22      False\n",
       "23      False\n",
       "24      False\n",
       "25      False\n",
       "26      False\n",
       "27      False\n",
       "28      False\n",
       "29      False\n",
       "        ...  \n",
       "2227    False\n",
       "2228    False\n",
       "2229    False\n",
       "2230    False\n",
       "2231    False\n",
       "2232    False\n",
       "2233    False\n",
       "2234    False\n",
       "2235    False\n",
       "2236    False\n",
       "2237    False\n",
       "2238    False\n",
       "2239    False\n",
       "2240    False\n",
       "2241    False\n",
       "2242    False\n",
       "2243    False\n",
       "2244    False\n",
       "2245    False\n",
       "2246    False\n",
       "2247    False\n",
       "2248    False\n",
       "2249    False\n",
       "2250    False\n",
       "2251    False\n",
       "2252    False\n",
       "2253    False\n",
       "2254    False\n",
       "2255    False\n",
       "2256    False\n",
       "Length: 2257, dtype: bool"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the sum of duplicated by simply doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that output, you may be asking why did the `duplicated` operation only returned one single column that indicates whether there is a duplicate record or not. So yes, all the operation does is to check per records instead of per column. That is why the operation only returns on value instead of three values for each columns, for each record as before. It appears that we don't have any duplicates since none of our records resulted in `True`. If we want to check for duplicates as we did above for some particular columns instead of all columns we do something as below. As you may have noticed, in this case where we select some columns instead of checking by all columns, we are kind of lowering the criteria of what is considered as a duplicate record. So let us only check for duplicateS by only the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X.duplicated('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create some dummy duplicate data and append it to the main dataframe `X`. And subsequenlty, let us try to get rid of the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_duplicate_dict = [{\n",
    "                             'text': 'dummy record',\n",
    "                             'category': 1, \n",
    "                             'category_name': \"dummy category\"\n",
    "                        },\n",
    "                        {\n",
    "                             'text': 'dummy record',\n",
    "                             'category': 1, \n",
    "                             'category_name': \"dummy category\"\n",
    "                        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.append(dummy_duplicate_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2259"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X.duplicated('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added the dummy duplicate to `X`. Now we are faced with the decision as to what to do with the duplicate after we have found it. In our case we want to get rid of all the duplicated records without preserving a copy. So we simple do that with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the Pandas [documentation](http://pandas.pydata.org/pandas-docs/stable/indexing.html?highlight=duplicate#duplicate-data) for more information on dealing with duplicate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Data Preprocessing\n",
    "In the Data Mining course we learned about the many ways of performing data preprocessing. In reality, the list is quiet general as the specifics of what data preprocessing involves is too much to cover in one course. This is especially true when you are dealing with unstructured data, as we are dealing with in this particular notebook. But let us look at some examples for each data preprocessing technique that we learned in the class. We will cover each item one by one, and provide example code for each category. You will learn how to peform each of the operations, using Pandas, that cover the essentials to Preprocessing in Data Mining. We are not going to follow any strict order, but the items we will cover in the preprocessing section of this notebook are as follows:\n",
    "\n",
    "- Aggregation\n",
    "- Sampling\n",
    "- Dimensionality Reduction\n",
    "- Feature Subset Selection\n",
    "- Feature Creation\n",
    "- Discretization and Binarization\n",
    "- Attribute Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sampling\n",
    "The first concept that we are going to cover from the above list is sampling. Sampling refers to the technique used for selecting data. The functionalities that we use to  selected data through queries provided by Pandas are actually methods for sampling. The reasons for sampling are sometimes due to the size of data; we want a smaller subset of the data that still best represents the complete dataset. We don't have a problem of size in our dataset since our dataset is just a couple thousand records long. But if we pay attention to how much content is included in the `text` field of each of those records, you will realize that sampling may not be a bad idea. In fact, we have already done some sampling by just reducing the records we are using here in this notebook; we only used four categories from the all the 20 categories available. Let us get an idea of what sampling involves in terms of using pandas code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something cool here while we are in sampling! Let us look at the distribution of categories in both the sample and original dataset. Let us visually analyze the disparity between the two datasets. To generate some visualizations, we are going to use `Plotly` python library. Plotly is an excellent visualization tool for data scientist to generate interactive charts and to create portable dashboards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_category_counts = ta.get_tokens_and_frequency(list(X.category_name))\n",
    "X_sample_category_counts = ta.get_tokens_and_frequency(list(X_sample.category_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~omarsar/124.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(ta.plot_word_frequency(X_category_counts, \"Category distribution\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~omarsar/126.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(ta.plot_word_frequency(X_sample_category_counts, \"Category distribution\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** We can also do a side by side comparison of the distribution between the two datasets, but maybe you can try that as an excerise. Look at the [Plotly documents](https://plot.ly/python/bar-charts/) for tons of examples and ways to visualizing bar charts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must also make mention that there are other libraries for visualizing, but we are going to focus on that in the later sections. For now, we just show you what are the ways to sample and to verify the distributions of your samplings. I can't resits it, but actually, there is an easier to generate the chart we generated above using a library called `matplotlib`. With matplotlib, things are faster and compatability-wise it may just be the best visualization library for visualizing content extracted from dataframes. Let's quickly take a loot at the magic of `matplotlib` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f26d62c7908>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noJekzv1/KJN5AzjaZkYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26d62c06d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.category_name.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f26d6355668>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA+BTsav0deh7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26d6357e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_sample.category_name.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that stood out from the both datasets, is that the distribution of the categories remain relatively the same, which is a good sign for us data scientist. For now that is. Actually there are many ways to conduct sampling on the dataset and still have a representative enough dataset. That is not the main focus in this notebook, but if you would like to know more about sampling and how the `sample` feature works, just reference the Pandas documentation and you will find interesting ways to conduct more advanced sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Creation\n",
    "The other operation from the list above that we are going to practise on is the so-called feature creation. As the name suggests, in feature creation we are looking at creating new interesting and useful features from the original dataset; a feature which captures the most important information from the raw information we already have access to. In our `X` table, we would like to create some features from the `text` field, but we are still not clear what kind of features we want to create. We can think of an interesting problem we want to solve, or something we want to analyze from the data, or some questions we want to answer. This is one process to come up with features -- this process is usually called `feature engineering` in the data science community. \n",
    "\n",
    "We know what feature creation is so let us get real involved with our dataset and make it more interesting by adding some special features or attributes if you will. First we are going to obtain the **unigrams** for each text. (Unigram is just a fancy word we use in Text Mining which stands for 'individual words'.) Yes, we want to extract all the words found in each text and append it as a new feature. The reason for extracting unigrams is not so clear yet, but we can start to think of obtaining some statistics about the articles we have: something like **word distribution** or **word frequency**.\n",
    "\n",
    "Before going into any further coding, we will also introduce some basic library called [NLTK](http://www.nltk.org/). The NLTK library is a natural language processing tool used for text mining tasks, so might as well we start to familiarize ourselves with it from now (It may come in handy for the final project!). In partcular, we are going to use the NLTK library to conduct tokenization because we are interested in splitting a sentence into its individual components, which can be words, emojis, emails, etc. So let us go for it! We can call the `nltk` library as follows:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a like a minute or two to process\n",
    "X['unigrams'] = X['text'].apply(lambda x: dmh.tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "      <th>bin_category</th>\n",
       "      <th>unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[From, :, sd345, @, city.ac.uk, (, Michael, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[From, :, ani, @, ms.uky.edu, (, Aniruddha, B....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "      <td>[From, :, djohnson, @, cs.ucsd.edu, (, Darin, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart) Subject...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "      <td>[From, :, s0612596, @, let.rug.nl, (, M.M, ., ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  \\\n",
       "0  From: sd345@city.ac.uk (Michael Collier) Subje...         1   \n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...         1   \n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...         3   \n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart) Subject...         3   \n",
       "\n",
       "            category_name  bin_category  \\\n",
       "0           comp.graphics  [0, 1, 0, 0]   \n",
       "1           comp.graphics  [0, 1, 0, 0]   \n",
       "2  soc.religion.christian  [0, 0, 0, 1]   \n",
       "3  soc.religion.christian  [0, 0, 0, 1]   \n",
       "\n",
       "                                            unigrams  \n",
       "0  [From, :, sd345, @, city.ac.uk, (, Michael, Co...  \n",
       "1  [From, :, ani, @, ms.uky.edu, (, Aniruddha, B....  \n",
       "2  [From, :, djohnson, @, cs.ucsd.edu, (, Darin, ...  \n",
       "3  [From, :, s0612596, @, let.rug.nl, (, M.M, ., ...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take at the `X` table now, you will see the new columns `unigrams` that we have added. You will notice that it contains an array of tokens, which were extracted from the original `text` field. On first impression, you will notice that the tokenizer is not doing a great job, let us take a closer at a single records and see what was the exact result of the tokenization using the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['From',\n",
       "  ':',\n",
       "  'sd345',\n",
       "  '@',\n",
       "  'city.ac.uk',\n",
       "  '(',\n",
       "  'Michael',\n",
       "  'Collier',\n",
       "  ')',\n",
       "  'Subject',\n",
       "  ':',\n",
       "  'Converting',\n",
       "  'images',\n",
       "  'to',\n",
       "  'HP',\n",
       "  'LaserJet',\n",
       "  'III',\n",
       "  '?',\n",
       "  'Nntp-Posting-Host',\n",
       "  ':',\n",
       "  'hampton',\n",
       "  'Organization',\n",
       "  ':',\n",
       "  'The',\n",
       "  'City',\n",
       "  'University',\n",
       "  'Lines',\n",
       "  ':',\n",
       "  '14',\n",
       "  'Does',\n",
       "  'anyone',\n",
       "  'know',\n",
       "  'of',\n",
       "  'a',\n",
       "  'good',\n",
       "  'way',\n",
       "  '(',\n",
       "  'standard',\n",
       "  'PC',\n",
       "  'application/PD',\n",
       "  'utility',\n",
       "  ')',\n",
       "  'to',\n",
       "  'convert',\n",
       "  'tif/img/tga',\n",
       "  'files',\n",
       "  'into',\n",
       "  'LaserJet',\n",
       "  'III',\n",
       "  'format',\n",
       "  '.',\n",
       "  'We',\n",
       "  'would',\n",
       "  'also',\n",
       "  'like',\n",
       "  'to',\n",
       "  'do',\n",
       "  'the',\n",
       "  'same',\n",
       "  ',',\n",
       "  'converting',\n",
       "  'to',\n",
       "  'HPGL',\n",
       "  '(',\n",
       "  'HP',\n",
       "  'plotter',\n",
       "  ')',\n",
       "  'files',\n",
       "  '.',\n",
       "  'Please',\n",
       "  'email',\n",
       "  'any',\n",
       "  'response',\n",
       "  '.',\n",
       "  'Is',\n",
       "  'this',\n",
       "  'the',\n",
       "  'correct',\n",
       "  'group',\n",
       "  '?',\n",
       "  'Thanks',\n",
       "  'in',\n",
       "  'advance',\n",
       "  '.',\n",
       "  'Michael',\n",
       "  '.',\n",
       "  '--',\n",
       "  'Michael',\n",
       "  'Collier',\n",
       "  '(',\n",
       "  'Programmer',\n",
       "  ')',\n",
       "  'The',\n",
       "  'Computer',\n",
       "  'Unit',\n",
       "  ',',\n",
       "  'Email',\n",
       "  ':',\n",
       "  'M.P.Collier',\n",
       "  '@',\n",
       "  'uk.ac.city',\n",
       "  'The',\n",
       "  'City',\n",
       "  'University',\n",
       "  ',',\n",
       "  'Tel',\n",
       "  ':',\n",
       "  '071',\n",
       "  '477-8000',\n",
       "  'x3769',\n",
       "  'London',\n",
       "  ',',\n",
       "  'Fax',\n",
       "  ':',\n",
       "  '071',\n",
       "  '477-8565',\n",
       "  'EC1V',\n",
       "  '0HB',\n",
       "  '.']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X[0:1]['unigrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` library does a pretty decent job of tokenizing our text. There are many other tokenizers online, such as [spacy](https://spacy.io/), and the built in libraries provided by [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). We are making use of the NLTK library because it is open source and because it simple does a good job of segmentating text-based data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature subset selection\n",
    "Okay, so we are making some headway here. Let us now make things a litte bit more interesting. We are going to do something different from what we have been doing thus far. We are going use a bit of everything that we have learned so far. Briefly speaking, we are going to move away from our main dataset (one form of feature subset selection), and we are going to generate a document-term matrix from the original dataset. In other words we are going to be creating something like this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://docs.google.com/drawings/d/e/2PACX-1vS01RrtPHS3r1Lf8UjX4POgDol-lVF4JAbjXM3SAOU-dOe-MqUdaEMWwJEPk9TtiUvcoSqTeE--lNep/pub?w=748&h=366)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, it won't have the same shape as the table above, but we will get into that later. For now, let us use scikit learn built in functionalities to generate this document. You will see for yourself how easy it is to generate this table without much coding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(X.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did with those two lines of code is that we transorfmed the articles into a term-document matrix. We can check the shape of this matrix throught the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '0000001200',\n",
       " '000005102000',\n",
       " '0001',\n",
       " '000100255pixel',\n",
       " '00014',\n",
       " '000406',\n",
       " '0007']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.get_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.fit(X.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['bin_category'] = mlb.transform(X['category']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "      <th>bin_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart) Subject...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: jodfishe@silver.ucs.indiana.edu (joseph ...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: aldridge@netcom.com (Jacquelin Aldridge)...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: geb@cs.pitt.edu (Gordon Banks) Subject: ...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  \\\n",
       "0  From: sd345@city.ac.uk (Michael Collier) Subje...         1   \n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...         1   \n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...         3   \n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart) Subject...         3   \n",
       "4  From: stanly@grok11.columbiasc.ncr.com (stanly...         3   \n",
       "5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...         3   \n",
       "6  From: jodfishe@silver.ucs.indiana.edu (joseph ...         3   \n",
       "7  From: aldridge@netcom.com (Jacquelin Aldridge)...         2   \n",
       "8  From: geb@cs.pitt.edu (Gordon Banks) Subject: ...         2   \n",
       "\n",
       "            category_name  bin_category  \n",
       "0           comp.graphics  [0, 1, 0, 0]  \n",
       "1           comp.graphics  [0, 1, 0, 0]  \n",
       "2  soc.religion.christian  [0, 0, 0, 1]  \n",
       "3  soc.religion.christian  [0, 0, 0, 1]  \n",
       "4  soc.religion.christian  [0, 0, 0, 1]  \n",
       "5  soc.religion.christian  [0, 0, 0, 1]  \n",
       "6  soc.religion.christian  [0, 0, 0, 1]  \n",
       "7                 sci.med  [0, 0, 1, 0]  \n",
       "8                 sci.med  [0, 0, 1, 0]  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f26dcadff98>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noJekzv1/KJN5AzjaZkYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26dcadf780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.category_name.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Nicely balanced dataset. Great news for classifiers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "      <th>bin_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier) Subje...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart) Subject...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: jodfishe@silver.ucs.indiana.edu (joseph ...</td>\n",
       "      <td>3</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: aldridge@netcom.com (Jacquelin Aldridge)...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: geb@cs.pitt.edu (Gordon Banks) Subject: ...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  \\\n",
       "0  From: sd345@city.ac.uk (Michael Collier) Subje...         1   \n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...         1   \n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...         3   \n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart) Subject...         3   \n",
       "4  From: stanly@grok11.columbiasc.ncr.com (stanly...         3   \n",
       "5  From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...         3   \n",
       "6  From: jodfishe@silver.ucs.indiana.edu (joseph ...         3   \n",
       "7  From: aldridge@netcom.com (Jacquelin Aldridge)...         2   \n",
       "8  From: geb@cs.pitt.edu (Gordon Banks) Subject: ...         2   \n",
       "\n",
       "            category_name  bin_category  \n",
       "0           comp.graphics  [0, 1, 0, 0]  \n",
       "1           comp.graphics  [0, 1, 0, 0]  \n",
       "2  soc.religion.christian  [0, 0, 0, 1]  \n",
       "3  soc.religion.christian  [0, 0, 0, 1]  \n",
       "4  soc.religion.christian  [0, 0, 0, 1]  \n",
       "5  soc.religion.christian  [0, 0, 0, 1]  \n",
       "6  soc.religion.christian  [0, 0, 0, 1]  \n",
       "7                 sci.med  [0, 0, 1, 0]  \n",
       "8                 sci.med  [0, 0, 1, 0]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas cook book ([Recommended for starters](http://pandas.pydata.org/pandas-docs/stable/cookbook.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
